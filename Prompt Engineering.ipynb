{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49abb756-a62c-444f-8e3f-a2078d0c55e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Basic Prompting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff523c2-1f23-4c58-99e4-9c8bbcb8e760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "166e98f6-2b73-4bea-9f81-7b1ce5e08736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09bb1c87-15a1-4761-a99e-a99f2c3c12e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key = \"your api key here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cf8d1c3-bd8c-4aae-acfa-b506757fc76a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def chat_completion(prompt, model):\n",
    "    input = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.responses.create(\n",
    "        model = model, \n",
    "        input = input)\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd046d73-7419-496e-b799-81c590efca8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"what was the result of FIFA worldcup ? \"\n",
    "\n",
    "chat_completion(prompt, \"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db6458b4-655c-4403-9147-9ef25368d5dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"what was the result of FIFA worldcup 2014 of women? \"\n",
    "\n",
    "chat_completion(prompt, \"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebbd5186-597d-4332-8251-a64e67032b78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Making prompt more specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1bca249-a5ec-4903-ae3e-19126f8709ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"what was the result of FIFA men's worldcup of 2014 ? return the result in a json format like following \n",
    "\n",
    "{\n",
    "  \"winner\": \"\",\n",
    "  \"runner_up\": \"\",\n",
    "  \"third_place\": \"\",\n",
    "  \"Golden boot\": \"\",\n",
    "  \"Golden Glove\": \"\",\n",
    "  \"Best Young Player\": \"\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "chat_completion(prompt, \"gpt-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42e7b047-d234-4503-a01b-f9a9d5aff504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Different Prompting techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c277655c-59dc-4dd5-8902-cc39b4842a8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Few Shot Prompting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25126dd4-d4fb-4cf9-a33d-cff08a07c2ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that classifies movie reviews as Positive or Negative.\"},\n",
    "\n",
    "        # Few-shot examples\n",
    "        {\"role\": \"user\", \"content\": \"Review: 'The movie was thrilling and kept me hooked till the end.'\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Positive\"},\n",
    "\n",
    "        {\"role\": \"user\", \"content\": \"Review: 'It was a complete waste of time. The acting was terrible.'\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Negative\"},\n",
    "\n",
    "        {\"role\": \"user\", \"content\": \"Review: 'An absolute masterpiece with brilliant cinematography.'\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Positive\"},\n",
    "\n",
    "        # Now the actual query\n",
    "        {\"role\": \"user\", \"content\": \"Review: 'The plot was weak and I almost fell asleep.'\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "def chat_completion(prompt, model):\n",
    "    # input = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.responses.create(\n",
    "        model = model, \n",
    "        input = prompt)\n",
    "    return response.output_text\n",
    "\n",
    "chat_completion(prompt, \"gpt-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62a3370a-98c0-4a9d-8f77-02432ede4b4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Chain of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "195ad19d-62dd-46f9-93ad-877ae0866812",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt =  \"\"\" \n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: The answer is 11.\n",
    "\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more how many apples do they have? \"\"\"\"\"\n",
    "\n",
    "chat_completion(prompt, \"gpt-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e962433a-6645-45f7-b077-88b3a8b6c73e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "\n",
    "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have? \"\"\"\n",
    "\n",
    "\n",
    "chat_completion(prompt, \"gpt-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a19e618-7d2f-4332-b84b-d3ec1c13fa06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Prompt Engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
